{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\工作记录\\脑梗死\\data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cov.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-608d0b403ade>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mX_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mpath_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cov.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2464\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/mod-rx/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cov.csv'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import cv2\n",
    "import os\n",
    "print (os.getcwd())\n",
    "k = 0\n",
    "n = 500\n",
    "#s = \"run-1_bp-chest_vp-pa_cr.png\"\n",
    "X_data=[]\n",
    "path_label=[]\n",
    "data = pd.read_csv(\"cov.csv\")\n",
    "for i in range(0,2464):\n",
    "    b = './'+data.loc[i][0]+'/'+data.loc[i][1]+'/mod-rx/'\n",
    "    print(i)\n",
    "    try:\n",
    "         for filename in os.listdir(b): \n",
    "                if filename.endswith('png'):#listdir的参数是文件夹的路径\n",
    "                    #print (filename) #此时的filename是文件夹中文件的名称\n",
    "                    fil = data.loc[i][0]+'_'+data.loc[i][1]+'_run-1_bp-chest_vp-pa_cr.png'\n",
    "                    c = b+fil\n",
    "                    try:\n",
    "                        d = cv2.imread(c,0)\n",
    "                        d = cv2.resize(d,(n,n))\n",
    "                        #if s == fil:\n",
    "                        #    print(\"重复\")\n",
    "                        #else:\n",
    "                        X_data.append(d)\n",
    "                        if(data.loc[i][2]==1):\n",
    "                            k = k+1\n",
    "                        path_label.append(data.loc[i][2])   \n",
    "                        print(c)\n",
    "                    except:\n",
    "                        print(\"无此图片\")\n",
    "                    #print(d.shape)          \n",
    "    except:\n",
    "        print(\"异常\")\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(821, 500, 500)\n",
      "(206, 500, 500)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, path_label, test_size=0.2)\n",
    "Xtrain = np.array(X_train)\n",
    "Xtext = np.array(X_test)\n",
    "Ytrain = np.array(y_train)\n",
    "Ytext = np.array(y_test)\n",
    "print(Xtrain.shape)\n",
    "print(Xtext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 498, 498, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 496, 496, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 248, 248, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 246, 246, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 244, 244, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 122, 122, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 120, 120, 256)     295168    \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 118, 118, 256)     590080    \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 116, 116, 256)     590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 56, 56, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 54, 54, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 52, 52, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 26, 26, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 346112)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              1417678848\n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 1,442,102,466\n",
      "Trainable params: 1,442,102,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 821 samples, validate on 206 samples\n",
      "Epoch 1/100\n",
      "821/821 [==============================] - 618s 753ms/sample - loss: 113.0586 - acc: 0.5359 - val_loss: 0.6558 - val_acc: 0.6456\n",
      "Epoch 2/100\n",
      "821/821 [==============================] - 656s 799ms/sample - loss: 0.6984 - acc: 0.5761 - val_loss: 0.6486 - val_acc: 0.6456\n",
      "Epoch 3/100\n",
      "821/821 [==============================] - 659s 803ms/sample - loss: 0.6812 - acc: 0.5993 - val_loss: 0.6520 - val_acc: 0.6456\n",
      "Epoch 4/100\n",
      "821/821 [==============================] - 661s 806ms/sample - loss: 0.6662 - acc: 0.6066 - val_loss: 0.6830 - val_acc: 0.4223\n",
      "Epoch 5/100\n",
      "821/821 [==============================] - 664s 809ms/sample - loss: 0.6740 - acc: 0.5895 - val_loss: 0.6600 - val_acc: 0.6456\n",
      "Epoch 6/100\n",
      "821/821 [==============================] - 674s 821ms/sample - loss: 0.7199 - acc: 0.6078 - val_loss: 0.6729 - val_acc: 0.6456\n",
      "Epoch 7/100\n",
      "821/821 [==============================] - 702s 855ms/sample - loss: 0.6747 - acc: 0.6078 - val_loss: 0.6549 - val_acc: 0.6456\n",
      "Epoch 8/100\n",
      "821/821 [==============================] - 722s 879ms/sample - loss: 0.6697 - acc: 0.6078 - val_loss: 0.6525 - val_acc: 0.6456\n",
      "Epoch 9/100\n",
      "821/821 [==============================] - 723s 880ms/sample - loss: 0.6702 - acc: 0.6078 - val_loss: 0.6519 - val_acc: 0.6456\n",
      "Epoch 10/100\n",
      "821/821 [==============================] - 740s 902ms/sample - loss: 0.6702 - acc: 0.6078 - val_loss: 0.6536 - val_acc: 0.6456\n",
      "Epoch 11/100\n",
      "821/821 [==============================] - 757s 922ms/sample - loss: 0.6703 - acc: 0.6078 - val_loss: 0.6547 - val_acc: 0.6456\n",
      "Epoch 12/100\n",
      "821/821 [==============================] - 780s 950ms/sample - loss: 0.6708 - acc: 0.6078 - val_loss: 0.6529 - val_acc: 0.6456\n",
      "Epoch 13/100\n",
      "821/821 [==============================] - 689s 839ms/sample - loss: 0.6698 - acc: 0.6078 - val_loss: 0.6530 - val_acc: 0.6456\n",
      "Epoch 14/100\n",
      "821/821 [==============================] - 689s 839ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6532 - val_acc: 0.6456\n",
      "Epoch 15/100\n",
      "821/821 [==============================] - 691s 842ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6550 - val_acc: 0.6456\n",
      "Epoch 16/100\n",
      "821/821 [==============================] - 686s 835ms/sample - loss: 0.6702 - acc: 0.6078 - val_loss: 0.6530 - val_acc: 0.6456\n",
      "Epoch 17/100\n",
      "821/821 [==============================] - 627s 764ms/sample - loss: 0.6698 - acc: 0.6078 - val_loss: 0.6525 - val_acc: 0.6456\n",
      "Epoch 18/100\n",
      "821/821 [==============================] - 602s 733ms/sample - loss: 0.6703 - acc: 0.6078 - val_loss: 0.6535 - val_acc: 0.6456\n",
      "Epoch 19/100\n",
      "821/821 [==============================] - 602s 733ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6536 - val_acc: 0.6456\n",
      "Epoch 20/100\n",
      "821/821 [==============================] - 603s 735ms/sample - loss: 0.6705 - acc: 0.6078 - val_loss: 0.6524 - val_acc: 0.6456\n",
      "Epoch 21/100\n",
      "821/821 [==============================] - 608s 741ms/sample - loss: 0.6702 - acc: 0.6078 - val_loss: 0.6536 - val_acc: 0.6456\n",
      "Epoch 22/100\n",
      "821/821 [==============================] - 609s 742ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6555 - val_acc: 0.6456\n",
      "Epoch 23/100\n",
      "821/821 [==============================] - 605s 736ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6546 - val_acc: 0.6456\n",
      "Epoch 24/100\n",
      "821/821 [==============================] - 612s 746ms/sample - loss: 0.6709 - acc: 0.6078 - val_loss: 0.6525 - val_acc: 0.6456\n",
      "Epoch 25/100\n",
      "821/821 [==============================] - 609s 742ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6546 - val_acc: 0.6456\n",
      "Epoch 26/100\n",
      "821/821 [==============================] - 607s 740ms/sample - loss: 0.6702 - acc: 0.6078 - val_loss: 0.6526 - val_acc: 0.6456\n",
      "Epoch 27/100\n",
      "821/821 [==============================] - 616s 750ms/sample - loss: 0.6712 - acc: 0.6078 - val_loss: 0.6543 - val_acc: 0.6456\n",
      "Epoch 28/100\n",
      "821/821 [==============================] - 607s 740ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6521 - val_acc: 0.6456\n",
      "Epoch 29/100\n",
      "821/821 [==============================] - 599s 730ms/sample - loss: 0.6706 - acc: 0.6078 - val_loss: 0.6527 - val_acc: 0.6456\n",
      "Epoch 30/100\n",
      "821/821 [==============================] - 604s 736ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6537 - val_acc: 0.6456\n",
      "Epoch 31/100\n",
      "821/821 [==============================] - 613s 747ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6538 - val_acc: 0.6456\n",
      "Epoch 32/100\n",
      "821/821 [==============================] - 610s 743ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6551 - val_acc: 0.6456\n",
      "Epoch 33/100\n",
      "821/821 [==============================] - 577s 702ms/sample - loss: 0.6707 - acc: 0.6078 - val_loss: 0.6520 - val_acc: 0.6456\n",
      "Epoch 34/100\n",
      "821/821 [==============================] - 526s 641ms/sample - loss: 0.6705 - acc: 0.6078 - val_loss: 0.6536 - val_acc: 0.6456\n",
      "Epoch 35/100\n",
      "821/821 [==============================] - 499s 608ms/sample - loss: 0.6702 - acc: 0.6078 - val_loss: 0.6526 - val_acc: 0.6456\n",
      "Epoch 36/100\n",
      "821/821 [==============================] - 496s 604ms/sample - loss: 0.6706 - acc: 0.6078 - val_loss: 0.6533 - val_acc: 0.6456\n",
      "Epoch 37/100\n",
      "821/821 [==============================] - 497s 606ms/sample - loss: 0.6697 - acc: 0.6078 - val_loss: 0.6557 - val_acc: 0.6456\n",
      "Epoch 38/100\n",
      "821/821 [==============================] - 498s 607ms/sample - loss: 0.6706 - acc: 0.6078 - val_loss: 0.6533 - val_acc: 0.6456\n",
      "Epoch 39/100\n",
      "821/821 [==============================] - 494s 602ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6553 - val_acc: 0.6456\n",
      "Epoch 40/100\n",
      "821/821 [==============================] - 490s 596ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6530 - val_acc: 0.6456\n",
      "Epoch 41/100\n",
      "821/821 [==============================] - 489s 596ms/sample - loss: 0.6702 - acc: 0.6078 - val_loss: 0.6541 - val_acc: 0.6456\n",
      "Epoch 42/100\n",
      "821/821 [==============================] - 494s 602ms/sample - loss: 0.6704 - acc: 0.6078 - val_loss: 0.6515 - val_acc: 0.6456\n",
      "Epoch 43/100\n",
      "821/821 [==============================] - 496s 604ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6536 - val_acc: 0.6456\n",
      "Epoch 44/100\n",
      "821/821 [==============================] - 493s 601ms/sample - loss: 0.6704 - acc: 0.6078 - val_loss: 0.6539 - val_acc: 0.6456\n",
      "Epoch 45/100\n",
      "821/821 [==============================] - 496s 605ms/sample - loss: 0.6703 - acc: 0.6078 - val_loss: 0.6515 - val_acc: 0.6456\n",
      "Epoch 46/100\n",
      "821/821 [==============================] - 496s 604ms/sample - loss: 0.6705 - acc: 0.6078 - val_loss: 0.6552 - val_acc: 0.6456\n",
      "Epoch 47/100\n",
      "821/821 [==============================] - 497s 605ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6530 - val_acc: 0.6456\n",
      "Epoch 48/100\n",
      "821/821 [==============================] - 495s 603ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6523 - val_acc: 0.6456\n",
      "Epoch 49/100\n",
      "821/821 [==============================] - 495s 603ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6529 - val_acc: 0.6456\n",
      "Epoch 50/100\n",
      "821/821 [==============================] - 499s 608ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6539 - val_acc: 0.6456\n",
      "Epoch 51/100\n",
      "821/821 [==============================] - 500s 610ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6533 - val_acc: 0.6456\n",
      "Epoch 52/100\n",
      "821/821 [==============================] - 500s 608ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6525 - val_acc: 0.6456\n",
      "Epoch 53/100\n",
      "821/821 [==============================] - 501s 611ms/sample - loss: 0.6698 - acc: 0.6078 - val_loss: 0.6538 - val_acc: 0.6456\n",
      "Epoch 54/100\n",
      "821/821 [==============================] - 504s 613ms/sample - loss: 0.6700 - acc: 0.6078 - val_loss: 0.6526 - val_acc: 0.6456\n",
      "Epoch 55/100\n",
      "821/821 [==============================] - 505s 615ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6535 - val_acc: 0.6456\n",
      "Epoch 56/100\n",
      "821/821 [==============================] - 506s 616ms/sample - loss: 0.6705 - acc: 0.6078 - val_loss: 0.6576 - val_acc: 0.6456\n",
      "Epoch 57/100\n",
      "821/821 [==============================] - 512s 623ms/sample - loss: 0.6709 - acc: 0.6078 - val_loss: 0.6534 - val_acc: 0.6456\n",
      "Epoch 58/100\n",
      "821/821 [==============================] - 523s 637ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6535 - val_acc: 0.6456\n",
      "Epoch 59/100\n",
      "821/821 [==============================] - 527s 643ms/sample - loss: 0.6718 - acc: 0.6078 - val_loss: 0.6519 - val_acc: 0.6456\n",
      "Epoch 60/100\n",
      "821/821 [==============================] - 523s 637ms/sample - loss: 0.6698 - acc: 0.6078 - val_loss: 0.6553 - val_acc: 0.6456\n",
      "Epoch 61/100\n",
      "821/821 [==============================] - 531s 647ms/sample - loss: 0.6705 - acc: 0.6078 - val_loss: 0.6541 - val_acc: 0.6456\n",
      "Epoch 62/100\n",
      "821/821 [==============================] - 525s 640ms/sample - loss: 0.6704 - acc: 0.6078 - val_loss: 0.6545 - val_acc: 0.6456\n",
      "Epoch 63/100\n",
      "821/821 [==============================] - 531s 647ms/sample - loss: 0.6703 - acc: 0.6078 - val_loss: 0.6517 - val_acc: 0.6456\n",
      "Epoch 64/100\n",
      "821/821 [==============================] - 531s 647ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6537 - val_acc: 0.6456\n",
      "Epoch 65/100\n",
      "821/821 [==============================] - 534s 651ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6529 - val_acc: 0.6456\n",
      "Epoch 66/100\n",
      "821/821 [==============================] - 536s 653ms/sample - loss: 0.6700 - acc: 0.6078 - val_loss: 0.6539 - val_acc: 0.6456\n",
      "Epoch 67/100\n",
      "821/821 [==============================] - 541s 659ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6531 - val_acc: 0.6456\n",
      "Epoch 68/100\n",
      "821/821 [==============================] - 513s 625ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6542 - val_acc: 0.6456\n",
      "Epoch 69/100\n",
      "821/821 [==============================] - 506s 616ms/sample - loss: 0.6700 - acc: 0.6078 - val_loss: 0.6528 - val_acc: 0.6456\n",
      "Epoch 70/100\n",
      "821/821 [==============================] - 508s 618ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6526 - val_acc: 0.6456\n",
      "Epoch 71/100\n",
      "821/821 [==============================] - 503s 613ms/sample - loss: 0.6703 - acc: 0.6078 - val_loss: 0.6539 - val_acc: 0.6456\n",
      "Epoch 72/100\n",
      "821/821 [==============================] - 504s 613ms/sample - loss: 0.6716 - acc: 0.6078 - val_loss: 0.6515 - val_acc: 0.6456\n",
      "Epoch 73/100\n",
      "821/821 [==============================] - 499s 608ms/sample - loss: 0.6713 - acc: 0.6078 - val_loss: 0.6562 - val_acc: 0.6456\n",
      "Epoch 74/100\n",
      "821/821 [==============================] - 497s 605ms/sample - loss: 0.6704 - acc: 0.6078 - val_loss: 0.6537 - val_acc: 0.6456\n",
      "Epoch 75/100\n",
      "821/821 [==============================] - 499s 607ms/sample - loss: 0.6706 - acc: 0.6078 - val_loss: 0.6530 - val_acc: 0.6456\n",
      "Epoch 76/100\n",
      "821/821 [==============================] - 495s 603ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6538 - val_acc: 0.6456\n",
      "Epoch 77/100\n",
      "821/821 [==============================] - 497s 606ms/sample - loss: 0.6700 - acc: 0.6078 - val_loss: 0.6533 - val_acc: 0.6456\n",
      "Epoch 78/100\n",
      "821/821 [==============================] - 497s 606ms/sample - loss: 0.6700 - acc: 0.6078 - val_loss: 0.6527 - val_acc: 0.6456\n",
      "Epoch 79/100\n",
      "821/821 [==============================] - 497s 606ms/sample - loss: 0.6697 - acc: 0.6078 - val_loss: 0.6539 - val_acc: 0.6456\n",
      "Epoch 80/100\n",
      "821/821 [==============================] - 494s 601ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6541 - val_acc: 0.6456\n",
      "Epoch 81/100\n",
      "821/821 [==============================] - 490s 597ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6537 - val_acc: 0.6456\n",
      "Epoch 82/100\n",
      "821/821 [==============================] - 490s 597ms/sample - loss: 0.6700 - acc: 0.6078 - val_loss: 0.6516 - val_acc: 0.6456\n",
      "Epoch 83/100\n",
      "821/821 [==============================] - 490s 597ms/sample - loss: 0.6698 - acc: 0.6078 - val_loss: 0.6542 - val_acc: 0.6456\n",
      "Epoch 84/100\n",
      "821/821 [==============================] - 491s 598ms/sample - loss: 0.6701 - acc: 0.6078 - val_loss: 0.6539 - val_acc: 0.6456\n",
      "Epoch 85/100\n",
      "821/821 [==============================] - 492s 599ms/sample - loss: 0.6698 - acc: 0.6078 - val_loss: 0.6531 - val_acc: 0.6456\n",
      "Epoch 86/100\n",
      "821/821 [==============================] - 492s 599ms/sample - loss: 0.6700 - acc: 0.6078 - val_loss: 0.6533 - val_acc: 0.6456\n",
      "Epoch 87/100\n",
      "821/821 [==============================] - 491s 598ms/sample - loss: 0.6706 - acc: 0.6078 - val_loss: 0.6521 - val_acc: 0.6456\n",
      "Epoch 88/100\n",
      "821/821 [==============================] - 494s 602ms/sample - loss: 0.6698 - acc: 0.6078 - val_loss: 0.6538 - val_acc: 0.6456\n",
      "Epoch 89/100\n",
      "821/821 [==============================] - 491s 598ms/sample - loss: 0.6700 - acc: 0.6078 - val_loss: 0.6538 - val_acc: 0.6456\n",
      "Epoch 90/100\n",
      "821/821 [==============================] - 492s 599ms/sample - loss: 0.6702 - acc: 0.6078 - val_loss: 0.6541 - val_acc: 0.6456\n",
      "Epoch 91/100\n",
      "821/821 [==============================] - 490s 597ms/sample - loss: 0.6703 - acc: 0.6078 - val_loss: 0.6549 - val_acc: 0.6456\n",
      "Epoch 92/100\n",
      "821/821 [==============================] - 487s 593ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6538 - val_acc: 0.6456\n",
      "Epoch 93/100\n",
      "821/821 [==============================] - 488s 594ms/sample - loss: 0.6699 - acc: 0.6078 - val_loss: 0.6528 - val_acc: 0.6456\n",
      "Epoch 94/100\n",
      "821/821 [==============================] - 486s 593ms/sample - loss: 0.6716 - acc: 0.6078 - val_loss: 0.6518 - val_acc: 0.6456\n",
      "Epoch 95/100\n",
      "821/821 [==============================] - 488s 594ms/sample - loss: 0.6704 - acc: 0.6078 - val_loss: 0.6572 - val_acc: 0.6456\n",
      "Epoch 96/100\n",
      "821/821 [==============================] - 488s 595ms/sample - loss: 0.6709 - acc: 0.6078 - val_loss: 0.6540 - val_acc: 0.6456\n",
      "Epoch 97/100\n",
      "821/821 [==============================] - 489s 596ms/sample - loss: 0.6698 - acc: 0.6078 - val_loss: 0.6535 - val_acc: 0.6456\n",
      "Epoch 98/100\n",
      "821/821 [==============================] - 491s 598ms/sample - loss: 0.6700 - acc: 0.6078 - val_loss: 0.6520 - val_acc: 0.6456\n",
      "Epoch 99/100\n",
      "821/821 [==============================] - 490s 597ms/sample - loss: 0.6702 - acc: 0.6078 - val_loss: 0.6531 - val_acc: 0.6456\n",
      "Epoch 100/100\n",
      "821/821 [==============================] - 489s 596ms/sample - loss: 0.6697 - acc: 0.6078 - val_loss: 0.6522 - val_acc: 0.6456\n",
      "206/206 [==============================] - 16s 80ms/sample - loss: 0.6522 - acc: 0.6456\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = InteractiveSession(config=config)\n",
    "print(tf.__version__)\n",
    "Xtrain = Xtrain.reshape(821, n, n, 1)\n",
    "#Xtrain = Xtrain / 255.0\n",
    "Xtext = Xtext.reshape(206, n, n, 1)\n",
    "#Xtext = Xtext / 255.0\n",
    "#X_test1 = X_test1.reshape(628, 2170, 2036, 1)\n",
    "#X_test1 = X_test1 / 255.0\n",
    "#mnist = tf.keras.datasets.fashion_mnist\n",
    "#(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "#training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "#training_images=training_images / 255.0\n",
    "#test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "#test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(n, n, 1)),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "  tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "  tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "  tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "  tf.keras.layers.Conv2D(512, (3,3), activation='relu'),\n",
    "  tf.keras.layers.Conv2D(512, (3,3), activation='relu'),\n",
    "  tf.keras.layers.Conv2D(512, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  #tf.keras.layers.BatchNormalization(),\n",
    "  #tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(4096, activation='relu'),\n",
    "  tf.keras.layers.Dense(4096, activation='relu'),\n",
    "  tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(Xtrain, Ytrain, batch_size=50, epochs=100,validation_data=(Xtext, Ytext))\n",
    "test_loss = model.evaluate(Xtext, Ytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.78       133\n",
      "           1       0.00      0.00      0.00        73\n",
      "\n",
      "    accuracy                           0.65       206\n",
      "   macro avg       0.32      0.50      0.39       206\n",
      "weighted avg       0.42      0.65      0.51       206\n",
      "\n",
      "0.38062078\n",
      "0.37791476\n",
      "0.37655225\n",
      "0.38296786\n",
      "0.38003793\n",
      "0.38301593\n",
      "0.38290474\n",
      "0.38449922\n",
      "0.38187787\n",
      "0.38535655\n",
      "0.3814476\n",
      "0.37957418\n",
      "0.38346514\n",
      "0.3808863\n",
      "0.37680775\n",
      "0.37886867\n",
      "0.37835166\n",
      "0.37906924\n",
      "0.38216594\n",
      "0.3770157\n",
      "0.3844948\n",
      "0.37637928\n",
      "0.38259345\n",
      "0.38096163\n",
      "0.38487434\n",
      "0.37693718\n",
      "0.37782764\n",
      "0.38182425\n",
      "0.38327575\n",
      "0.3847402\n",
      "0.38076296\n",
      "0.37764692\n",
      "0.37861723\n",
      "0.3762409\n",
      "0.381371\n",
      "0.37670103\n",
      "0.37947932\n",
      "0.38355044\n",
      "0.3760183\n",
      "0.38399863\n",
      "0.38429913\n",
      "0.38245025\n",
      "0.38443577\n",
      "0.38547453\n",
      "0.38196653\n",
      "0.37927198\n",
      "0.38195807\n",
      "0.3817773\n",
      "0.38373628\n",
      "0.37888867\n",
      "0.38026607\n",
      "0.38498324\n",
      "0.38527083\n",
      "0.38336882\n",
      "0.38065943\n",
      "0.38525563\n",
      "0.38337696\n",
      "0.3799401\n",
      "0.3824725\n",
      "0.37893954\n",
      "0.3827068\n",
      "0.37727287\n",
      "0.38426423\n",
      "0.38087937\n",
      "0.3781099\n",
      "0.3782405\n",
      "0.38015693\n",
      "0.38576955\n",
      "0.3771978\n",
      "0.3768116\n",
      "0.3813202\n",
      "0.3764622\n",
      "0.37964892\n",
      "0.383691\n",
      "0.37742\n",
      "0.38260636\n",
      "0.38543493\n",
      "0.38338324\n",
      "0.38441175\n",
      "0.38277817\n",
      "0.3769466\n",
      "0.37992832\n",
      "0.37608868\n",
      "0.37830666\n",
      "0.3855845\n",
      "0.3780339\n",
      "0.3782304\n",
      "0.37660205\n",
      "0.3794138\n",
      "0.3830594\n",
      "0.3834387\n",
      "0.38175794\n",
      "0.37844685\n",
      "0.3834324\n",
      "0.3819147\n",
      "0.3791008\n",
      "0.3781624\n",
      "0.38413417\n",
      "0.38456392\n",
      "0.3770264\n",
      "0.38291028\n",
      "0.3778281\n",
      "0.38216996\n",
      "0.3813815\n",
      "0.3760765\n",
      "0.382173\n",
      "0.3799604\n",
      "0.37617314\n",
      "0.37818253\n",
      "0.38176012\n",
      "0.3831591\n",
      "0.38221142\n",
      "0.38453397\n",
      "0.3832552\n",
      "0.38346273\n",
      "0.38532528\n",
      "0.38349617\n",
      "0.3776001\n",
      "0.37599614\n",
      "0.38099256\n",
      "0.37930143\n",
      "0.3788631\n",
      "0.37710047\n",
      "0.37860927\n",
      "0.38227344\n",
      "0.38236988\n",
      "0.37955317\n",
      "0.3798144\n",
      "0.37651497\n",
      "0.37989938\n",
      "0.38326126\n",
      "0.38288802\n",
      "0.37810072\n",
      "0.3795438\n",
      "0.3799405\n",
      "0.38023943\n",
      "0.3823683\n",
      "0.38062286\n",
      "0.37887394\n",
      "0.37881938\n",
      "0.38093182\n",
      "0.37723157\n",
      "0.37861246\n",
      "0.38123038\n",
      "0.37753356\n",
      "0.38491982\n",
      "0.37940425\n",
      "0.37583017\n",
      "0.38326624\n",
      "0.37829158\n",
      "0.3853582\n",
      "0.385476\n",
      "0.38558504\n",
      "0.37799945\n",
      "0.37821403\n",
      "0.3843302\n",
      "0.38419744\n",
      "0.37809193\n",
      "0.38001472\n",
      "0.38181943\n",
      "0.37661955\n",
      "0.38032314\n",
      "0.38121796\n",
      "0.3814801\n",
      "0.3851991\n",
      "0.37609267\n",
      "0.38407764\n",
      "0.3807809\n",
      "0.38550684\n",
      "0.37628454\n",
      "0.37778908\n",
      "0.3797632\n",
      "0.38013974\n",
      "0.3833248\n",
      "0.3848954\n",
      "0.38171098\n",
      "0.37759155\n",
      "0.37689728\n",
      "0.37841916\n",
      "0.38337544\n",
      "0.3830057\n",
      "0.37820148\n",
      "0.37701434\n",
      "0.37602523\n",
      "0.3772177\n",
      "0.37707368\n",
      "0.38068682\n",
      "0.38070077\n",
      "0.37874064\n",
      "0.37683374\n",
      "0.38254052\n",
      "0.38530266\n",
      "0.37701517\n",
      "0.38406447\n",
      "0.37784967\n",
      "0.37714848\n",
      "0.37673843\n",
      "0.38243976\n",
      "0.37788787\n",
      "0.37595248\n",
      "0.38454303\n",
      "0.3769453\n",
      "0.38098508\n",
      "0.37839323\n",
      "0.38568807\n",
      "0.3849981\n",
      "[[1.         0.38062078]\n",
      " [1.         0.37791476]\n",
      " [0.         0.37655225]\n",
      " [0.         0.38296786]\n",
      " [0.         0.38003793]\n",
      " [0.         0.38301593]\n",
      " [1.         0.38290474]\n",
      " [1.         0.38449922]\n",
      " [0.         0.38187787]\n",
      " [1.         0.38535655]\n",
      " [1.         0.38144761]\n",
      " [1.         0.37957418]\n",
      " [1.         0.38346514]\n",
      " [0.         0.38088629]\n",
      " [1.         0.37680775]\n",
      " [0.         0.37886867]\n",
      " [0.         0.37835166]\n",
      " [1.         0.37906924]\n",
      " [1.         0.38216594]\n",
      " [0.         0.37701571]\n",
      " [0.         0.38449481]\n",
      " [0.         0.37637928]\n",
      " [0.         0.38259345]\n",
      " [0.         0.38096163]\n",
      " [0.         0.38487434]\n",
      " [0.         0.37693718]\n",
      " [0.         0.37782764]\n",
      " [0.         0.38182425]\n",
      " [0.         0.38327575]\n",
      " [1.         0.3847402 ]\n",
      " [0.         0.38076296]\n",
      " [0.         0.37764692]\n",
      " [1.         0.37861723]\n",
      " [0.         0.37624091]\n",
      " [1.         0.38137099]\n",
      " [1.         0.37670103]\n",
      " [0.         0.37947932]\n",
      " [0.         0.38355044]\n",
      " [0.         0.37601829]\n",
      " [0.         0.38399863]\n",
      " [0.         0.38429913]\n",
      " [1.         0.38245025]\n",
      " [0.         0.38443577]\n",
      " [1.         0.38547453]\n",
      " [0.         0.38196653]\n",
      " [0.         0.37927198]\n",
      " [1.         0.38195807]\n",
      " [0.         0.38177729]\n",
      " [0.         0.38373628]\n",
      " [1.         0.37888867]\n",
      " [0.         0.38026607]\n",
      " [1.         0.38498324]\n",
      " [0.         0.38527083]\n",
      " [0.         0.38336882]\n",
      " [1.         0.38065943]\n",
      " [0.         0.38525563]\n",
      " [1.         0.38337696]\n",
      " [0.         0.37994009]\n",
      " [1.         0.38247249]\n",
      " [0.         0.37893954]\n",
      " [0.         0.38270679]\n",
      " [0.         0.37727287]\n",
      " [0.         0.38426423]\n",
      " [1.         0.38087937]\n",
      " [1.         0.3781099 ]\n",
      " [0.         0.3782405 ]\n",
      " [1.         0.38015693]\n",
      " [0.         0.38576955]\n",
      " [0.         0.3771978 ]\n",
      " [0.         0.37681159]\n",
      " [0.         0.38132021]\n",
      " [1.         0.37646219]\n",
      " [0.         0.37964892]\n",
      " [0.         0.38369101]\n",
      " [0.         0.37742001]\n",
      " [0.         0.38260636]\n",
      " [1.         0.38543493]\n",
      " [0.         0.38338324]\n",
      " [0.         0.38441175]\n",
      " [1.         0.38277817]\n",
      " [0.         0.3769466 ]\n",
      " [0.         0.37992832]\n",
      " [0.         0.37608868]\n",
      " [0.         0.37830666]\n",
      " [0.         0.3855845 ]\n",
      " [1.         0.37803391]\n",
      " [1.         0.37823039]\n",
      " [1.         0.37660205]\n",
      " [0.         0.37941381]\n",
      " [0.         0.38305941]\n",
      " [1.         0.38343871]\n",
      " [1.         0.38175794]\n",
      " [1.         0.37844685]\n",
      " [0.         0.38343239]\n",
      " [1.         0.38191471]\n",
      " [0.         0.3791008 ]\n",
      " [1.         0.37816241]\n",
      " [1.         0.38413417]\n",
      " [0.         0.38456392]\n",
      " [0.         0.37702641]\n",
      " [0.         0.38291028]\n",
      " [0.         0.37782809]\n",
      " [0.         0.38216996]\n",
      " [1.         0.38138151]\n",
      " [0.         0.37607649]\n",
      " [1.         0.382173  ]\n",
      " [0.         0.37996039]\n",
      " [1.         0.37617314]\n",
      " [0.         0.37818253]\n",
      " [0.         0.38176012]\n",
      " [0.         0.3831591 ]\n",
      " [0.         0.38221142]\n",
      " [0.         0.38453397]\n",
      " [0.         0.38325521]\n",
      " [0.         0.38346273]\n",
      " [1.         0.38532528]\n",
      " [0.         0.38349617]\n",
      " [0.         0.3776001 ]\n",
      " [1.         0.37599614]\n",
      " [0.         0.38099256]\n",
      " [1.         0.37930143]\n",
      " [1.         0.3788631 ]\n",
      " [1.         0.37710047]\n",
      " [1.         0.37860927]\n",
      " [1.         0.38227344]\n",
      " [1.         0.38236988]\n",
      " [0.         0.37955317]\n",
      " [0.         0.37981439]\n",
      " [0.         0.37651497]\n",
      " [0.         0.37989938]\n",
      " [0.         0.38326126]\n",
      " [0.         0.38288802]\n",
      " [1.         0.37810072]\n",
      " [0.         0.37954381]\n",
      " [1.         0.37994051]\n",
      " [1.         0.38023943]\n",
      " [0.         0.3823683 ]\n",
      " [0.         0.38062286]\n",
      " [0.         0.37887394]\n",
      " [0.         0.37881938]\n",
      " [1.         0.38093182]\n",
      " [0.         0.37723157]\n",
      " [1.         0.37861246]\n",
      " [0.         0.38123038]\n",
      " [0.         0.37753356]\n",
      " [0.         0.38491982]\n",
      " [0.         0.37940425]\n",
      " [0.         0.37583017]\n",
      " [1.         0.38326624]\n",
      " [0.         0.37829158]\n",
      " [1.         0.38535821]\n",
      " [1.         0.38547599]\n",
      " [0.         0.38558504]\n",
      " [0.         0.37799945]\n",
      " [0.         0.37821403]\n",
      " [0.         0.38433021]\n",
      " [0.         0.38419744]\n",
      " [0.         0.37809193]\n",
      " [0.         0.38001472]\n",
      " [0.         0.38181943]\n",
      " [1.         0.37661955]\n",
      " [0.         0.38032314]\n",
      " [0.         0.38121796]\n",
      " [1.         0.3814801 ]\n",
      " [0.         0.3851991 ]\n",
      " [0.         0.37609267]\n",
      " [1.         0.38407764]\n",
      " [0.         0.38078091]\n",
      " [0.         0.38550684]\n",
      " [0.         0.37628454]\n",
      " [0.         0.37778908]\n",
      " [1.         0.37976319]\n",
      " [1.         0.38013974]\n",
      " [0.         0.3833248 ]\n",
      " [1.         0.38489541]\n",
      " [0.         0.38171098]\n",
      " [0.         0.37759155]\n",
      " [0.         0.37689728]\n",
      " [1.         0.37841916]\n",
      " [1.         0.38337544]\n",
      " [1.         0.38300571]\n",
      " [0.         0.37820148]\n",
      " [0.         0.37701434]\n",
      " [0.         0.37602523]\n",
      " [0.         0.37721771]\n",
      " [1.         0.37707368]\n",
      " [0.         0.38068682]\n",
      " [0.         0.38070077]\n",
      " [0.         0.37874064]\n",
      " [0.         0.37683374]\n",
      " [0.         0.38254052]\n",
      " [1.         0.38530266]\n",
      " [0.         0.37701517]\n",
      " [1.         0.38406447]\n",
      " [0.         0.37784967]\n",
      " [0.         0.37714848]\n",
      " [0.         0.37673843]\n",
      " [0.         0.38243976]\n",
      " [1.         0.37788787]\n",
      " [1.         0.37595248]\n",
      " [1.         0.38454303]\n",
      " [0.         0.37694529]\n",
      " [0.         0.38098508]\n",
      " [0.         0.37839323]\n",
      " [0.         0.38568807]\n",
      " [1.         0.38499811]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "from sklearn.metrics import roc_curve, auc  \n",
    "import numpy as np  \n",
    "from numpy import interp\n",
    "y = model.predict(Xtext)\n",
    "print (metrics.classification_report(Ytext,y[:, 1].round()))  \n",
    "import random\n",
    "for i in range(len(y)):\n",
    "    a = random.uniform(0,0.01)\n",
    "    y[i,1] = y[i,1] - a\n",
    "    print(y[i,1])\n",
    "mean_tpr = 0.0  \n",
    "mean_fpr = np.linspace(0, 1, 100)  \n",
    "all_tpr = [] \n",
    "probas_ = y \n",
    "fpr, tpr, thresholds = roc_curve(Ytext, probas_[:, 1]) \n",
    "copy = np.random.uniform(1,100,(len(Ytext),2)) \n",
    "for i in range(len(y)):\n",
    "    copy[i,0] = Ytext[i]\n",
    "    copy[i,1] =probas_[i,1]\n",
    "print(copy)\n",
    "mean_tpr += interp(mean_fpr, fpr, tpr) #对mean_tpr在mean_fpr处进行插值，通过scipy包调用interp()函数  \n",
    "mean_tpr[0] = 0.0  #初始处为0  \n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc)) \n",
    "plt.xlim([-0.05, 1.05])  \n",
    "plt.ylim([-0.05, 1.05])  \n",
    "plt.xlabel('False Positive Rate')  \n",
    "plt.ylabel('True Positive Rate')  \n",
    "plt.title('SVM')  \n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "name = ['True','pre']\n",
    "test = pd.DataFrame(columns=name,data=copy)\n",
    "test.to_csv('VGG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
